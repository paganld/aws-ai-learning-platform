"""
AWS AI Learning Platform - RAG Backend
FastAPI server with Google Gemini and ChromaDB
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import os
from dotenv import load_dotenv

# LangChain imports
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
import google.generativeai as genai

# Load environment variables
load_dotenv()

app = FastAPI(title="AWS AI Learning Platform API")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Next.js dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables
vector_store = None
qa_chain = None
embeddings = None
llm = None

# Request/Response Models
class ChatRequest(BaseModel):
    question: str
    conversation_history: Optional[List[dict]] = []

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]
    confidence: Optional[float] = None

class QuizRequest(BaseModel):
    topic: str
    difficulty: str = "medium"  # easy, medium, hard
    num_questions: int = 5

class QuizResponse(BaseModel):
    questions: List[dict]

class DocumentInfo(BaseModel):
    title: str
    url: str
    service: str
    category: str


# Initialize RAG system
def initialize_rag():
    """Initialize the RAG system with ChromaDB and Gemini"""
    global vector_store, qa_chain, embeddings, llm

    # Check for API key
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY not found in environment variables")

    # Initialize embeddings - using HuggingFace (local, no API quotas!)
    from langchain_community.embeddings import HuggingFaceEmbeddings
    print("ðŸ”§ Loading local embedding model...")
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… Embedding model loaded!")

    # Initialize LLM
    llm = GoogleGenerativeAI(
        model="gemini-pro",
        google_api_key=api_key,
        temperature=float(os.getenv("TEMPERATURE", 0.7)),
        max_output_tokens=int(os.getenv("MAX_TOKENS", 2048))
    )

    # Initialize ChromaDB
    persist_directory = os.getenv("CHROMA_PERSIST_DIRECTORY", "./chroma_db")

    try:
        vector_store = Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings,
            collection_name="aws_docs"
        )
        print(f"âœ… Loaded vector store with {vector_store._collection.count()} documents")
    except Exception as e:
        print(f"âš ï¸  Creating new vector store: {e}")
        vector_store = Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings,
            collection_name="aws_docs"
        )

    # Create custom prompt template
    prompt_template = """You are an expert AWS AI/ML instructor helping students prepare for AWS certifications including:
- AWS Certified AI Practitioner
- AWS Certified Machine Learning - Specialty
- AWS AI/ML related services

Use the following context from AWS documentation to answer the question. If you don't know the answer based on the context, say so clearly.

Context:
{context}

Question: {question}

Provide a clear, detailed answer that:
1. Directly answers the question
2. Includes relevant AWS service names and features
3. Explains concepts in an educational way
4. Relates to certification exam topics when relevant

Answer:"""

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # Create QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        ),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

    print("âœ… RAG system initialized successfully")


@app.on_event("startup")
async def startup_event():
    """Initialize RAG on startup"""
    try:
        initialize_rag()
    except Exception as e:
        print(f"âŒ Error initializing RAG: {e}")
        print("âš ï¸  Server running but RAG not available. Please check configuration.")


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "online",
        "message": "AWS AI Learning Platform API",
        "rag_initialized": qa_chain is not None,
        "documents_loaded": vector_store._collection.count() if vector_store else 0
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint for RAG-powered Q&A
    """
    if not qa_chain:
        raise HTTPException(status_code=503, detail="RAG system not initialized")

    try:
        # Run the query through RAG
        result = qa_chain({"query": request.question})

        # Extract sources
        sources = []
        if "source_documents" in result:
            for doc in result["source_documents"]:
                if hasattr(doc, 'metadata'):
                    source = doc.metadata.get('source', 'Unknown')
                    if source not in sources:
                        sources.append(source)

        return ChatResponse(
            answer=result["result"],
            sources=sources[:3]  # Top 3 sources
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing question: {str(e)}")


@app.post("/quiz", response_model=QuizResponse)
async def generate_quiz(request: QuizRequest):
    """
    Generate a quiz on a specific AWS AI/ML topic
    """
    if not llm:
        raise HTTPException(status_code=503, detail="LLM not initialized")

    try:
        # Create quiz generation prompt
        quiz_prompt = f"""Generate a {request.difficulty} difficulty quiz with {request.num_questions} multiple choice questions about {request.topic} in AWS.

Focus on topics relevant to AWS AI Practitioner and Machine Learning certifications.

Format the response as a JSON array with this structure:
[
  {{
    "question": "Question text here?",
    "options": ["A) Option 1", "B) Option 2", "C) Option 3", "D) Option 4"],
    "correct_answer": "A",
    "explanation": "Brief explanation of why this is correct"
  }}
]

Make questions practical and exam-relevant."""

        # Generate quiz using LLM
        response = llm.invoke(quiz_prompt)

        # Parse response (you may need to clean this up)
        import json
        import re

        # Extract JSON from response
        json_match = re.search(r'\[.*\]', response, re.DOTALL)
        if json_match:
            questions = json.loads(json_match.group())
        else:
            # Fallback: create a simple question
            questions = [{
                "question": f"What is the primary use case for {request.topic}?",
                "options": ["A) Data storage", "B) Machine learning", "C) Networking", "D) Security"],
                "correct_answer": "B",
                "explanation": "Please try again - quiz generation needs refinement"
            }]

        return QuizResponse(questions=questions)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating quiz: {str(e)}")


@app.get("/topics")
async def get_topics():
    """
    Get available AWS AI/ML topics from the knowledge base
    """
    topics = {
        "ai_services": [
            "Amazon SageMaker",
            "Amazon Bedrock",
            "Amazon Q",
            "Amazon CodeWhisperer",
            "Amazon Comprehend",
            "Amazon Rekognition",
            "Amazon Textract",
            "Amazon Translate",
            "Amazon Polly",
            "Amazon Transcribe",
            "Amazon Lex",
            "Amazon Personalize"
        ],
        "ml_infrastructure": [
            "SageMaker Studio",
            "SageMaker Training",
            "SageMaker Inference",
            "SageMaker Feature Store",
            "SageMaker Model Monitor",
            "SageMaker Autopilot",
            "SageMaker Neo",
            "Elastic Inference"
        ],
        "certifications": [
            "AWS Certified AI Practitioner",
            "AWS Certified Machine Learning - Specialty"
        ]
    }

    return topics


@app.get("/stats")
async def get_stats():
    """
    Get statistics about the knowledge base
    """
    if not vector_store:
        return {"error": "Vector store not initialized"}

    try:
        count = vector_store._collection.count()
        return {
            "total_documents": count,
            "status": "healthy" if count > 0 else "needs_documents",
            "embedding_model": "models/embedding-001",
            "llm_model": "gemini-pro"
        }
    except Exception as e:
        return {"error": str(e)}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
